import boto3                                # Library necessary for bucket manipulation
import os                                   # Library necessary for filesystem manipulation
from avro.datafile import DataFileReader    # Library necessary for avro-to-csv conversion
from avro.io import DatumReader             # Library necessary for avro-to-csv conversion
import json                                 # Library necessary for avro-to-csv conversion
import csv                                  # Library necessary for avro-to-csv conversion
import shutil                               # Needed for file deletion

# Declaring Amazon Bucket Personal Variables, They Are Manually Given In Script File
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID') # Id obtained from Care Website
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY') # Key obtained from Care Website
bucket_name = os.getenv('BUCKET_NAME') # Bucket Name of the AWS Bucket
prefix_version=os.getenv('PREFIX_VERSION') # First v2 in the bucket path
organization_id=os.getenv('ORGANIZATION_ID') # Organization Id in the Care Website
prefix_path_first_unknown_number=os.getenv('PREFIX_PATH_FIRST_UNKNOWN_NUMBER') # Number after the Organization ID (probably the supervisor ID)
prefix_path_second_unknown_number=os.getenv('PREFIX_PATH_SECOND_UNKNOWN_NUMBER') # Number after the supervisor ID 
date_of_stats=os.getenv('DATE_OF_STATS') # Date in format of YYYY-MM-DD
participant_info=os.getenv('PARTICIPANT_INFO') # Participant ID and info generated by Care Website
raw_data_version=os.getenv('RAW_DATA_VERSION') # No idea but it is 6 for now.

# Save Directory For Raw Data In Computer
raw_data_save_directory = os.getenv('RAW_DATA_SAVE_DIRECTORY')

# Save Directory For CSV_files Computer
csv_save_directory = os.getenv('CSV_SAVE_DIRECTORY')

# Creation of Full Path For Raw Data In Bucket
full_path = prefix_version+"/"+organization_id+"/"+prefix_path_first_unknown_number+"/"+prefix_path_second_unknown_number+"/participant_data/"+date_of_stats+"/"+participant_info+"/raw_data/"+raw_data_version+"/"

# Creation of Full Path For CSV Files Prepared by Care
full_path_care_csv = prefix_version+"/"+organization_id+"/"+prefix_path_first_unknown_number+"/"+prefix_path_second_unknown_number+"/participant_data/"+date_of_stats+"/"+participant_info+"/digital_biomarkers/aggregated_per_minute/"

# Function to delete everything in a specific path (Needed to delete old avro files)
def delete_contents(path):
    if os.path.exists(path) and os.path.isdir(path):
        shutil.rmtree(path)
        os.makedirs(path)
    else:
        os.makedirs(path)

# Delete old raw data, create the path if not exists
delete_contents(raw_data_save_directory)

# Create the csv folder if not exists already
if not os.path.exists(csv_save_directory):
    os.makedirs(csv_save_directory)

# Start the bucket connection
s3 = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
)

# Obtain the response for listing the raw data
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=full_path)

# Get the latest avro file, also its last update date    
newest_file = max(response['Contents'], key=lambda obj: obj['LastModified'])
file_key = newest_file['Key']
last_modified = newest_file['LastModified']
local_file_path = os.path.join(raw_data_save_directory, os.path.basename(file_key))
s3.download_file(bucket_name, file_key, local_file_path)

# Obtain the response for listing the care prepared csv data
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=full_path_care_csv)

# Download all the care prepared csv data
for obj in response['Contents']:
    file_key = obj['Key']
    local_file_path = os.path.join(csv_save_directory, os.path.basename(file_key))
    s3.download_file(bucket_name, file_key, local_file_path)

# Print the info
print(f"Latest avro file has been downloaded: {file_key}")
print(f"Latest raw data update: {last_modified} (The timezone may differ!!!)")

# Find the full path of the avro file (as the name changes every time, it is dynamic)
def find_single_avro_file(directory):
    txt_files = [f for f in os.listdir(directory) if f.endswith('.avro')]
    return os.path.join(directory, txt_files[0])

# Call the finder func
avro_file_path = find_single_avro_file(raw_data_save_directory)

## Read Avro file
reader = DataFileReader(open(avro_file_path, "rb"), DatumReader())
schema = json.loads(reader.meta.get('avro.schema').decode('utf-8'))
data= next(reader)

## Export sensors data to csv files
# Accelerometer
acc = data["rawData"]["accelerometer"]
timestamp = [round(acc["timestampStart"] + i * (1e6 / acc["samplingFrequency"]))
            for i in range(len(acc["x"]))]
# Convert ADC counts in g
delta_physical = acc["imuParams"]["physicalMax"] - acc["imuParams"]["physicalMin"]
delta_digital = acc["imuParams"]["digitalMax"] - acc["imuParams"]["digitalMin"]
x_g = [val * delta_physical / delta_digital for val in acc["x"]]
y_g = [val * delta_physical / delta_digital for val in acc["y"]]
z_g = [val * delta_physical / delta_digital for val in acc["z"]]
with open(os.path.join(csv_save_directory, 'accelerometer.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["unix_timestamp", "x", "y", "z"])
    writer.writerows([[ts, x, y, z] for ts, x, y, z in zip(timestamp, x_g, y_g, z_g)])

# Gyroscope
gyro = data["rawData"]["gyroscope"]
timestamp = [round(gyro["timestampStart"] + i * (1e6 / gyro["samplingFrequency"]))
            for i in range(len(gyro["x"]))]
with open(os.path.join(csv_save_directory, 'gyroscope.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["unix_timestamp", "x", "y", "z"])
    writer.writerows([[ts, x, y, z] for ts, x, y, z in zip(timestamp, gyro["x"], gyro["y"], gyro["z"])])

# Eda
eda = data["rawData"]["eda"]
timestamp = [round(eda["timestampStart"] + i * (1e6 / eda["samplingFrequency"]))
            for i in range(len(eda["values"]))]
with open(os.path.join(csv_save_directory, 'eda.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["unix_timestamp", "eda"])
    writer.writerows([[ts, eda] for ts, eda in zip(timestamp, eda["values"])])

# Temperature
tmp = data["rawData"]["temperature"]
timestamp = [round(tmp["timestampStart"] + i * (1e6 / tmp["samplingFrequency"]))
    for i in range(len(tmp["values"]))]
with open(os.path.join(csv_save_directory, 'temperature.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["unix_timestamp", "temperature"])
    writer.writerows([[ts, tmp] for ts, tmp in zip(timestamp, tmp["values"])])

# Tags
tags = data["rawData"]["tags"]
with open(os.path.join(csv_save_directory, 'tags.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["tags_timestamp"])
    writer.writerows([[tag] for tag in tags["tagsTimeMicros"]])

# BVP
bvp = data["rawData"]["bvp"]
timestamp = [round(bvp["timestampStart"] + i * (1e6 / bvp["samplingFrequency"]))
            for i in range(len(bvp["values"]))]
with open(os.path.join(csv_save_directory, 'bvp.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["unix_timestamp", "bvp"])
    writer.writerows([[ts, bvp] for ts, bvp in zip(timestamp, bvp["values"])])

# Systolic peaks
sps = data["rawData"]["systolicPeaks"]
with open(os.path.join(csv_save_directory, 'systolic_peaks.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["systolic_peak_timestamp"])
    writer.writerows([[sp] for sp in sps["peaksTimeNanos"]])

# Steps
steps = data["rawData"]["steps"]
timestamp = [round(steps["timestampStart"] + i * (1e6 / steps["samplingFrequency"]))
            for i in range(len(steps["values"]))]
with open(os.path.join(csv_save_directory, 'steps.csv'), 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["unix_timestamp", "steps"])
    writer.writerows([[ts, step] for ts, step in zip(timestamp, steps["values"])])
